{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Precision_and_Recall.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGPRkqjnIVy4nUbWzakDNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aviguptatx/GansResearch/blob/master/Precision_and_Recall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaGgR4ptK0s4"
      },
      "source": [
        "This code is based off of https://github.com/blandocs/improved-precision-and-recall-metric-pytorch (with a few modifications)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52yX9sP3G9dT"
      },
      "source": [
        "import os, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class feature_extractor(object):\n",
        "    def __init__(self, args):\n",
        "        # parameters\n",
        "        self.args = args\n",
        "        self.generated_dir = args.generated_dir\n",
        "        self.real_dir = args.real_dir\n",
        "        self.batch_size = args.batch_size\n",
        "        self.cpu = args.cpu\n",
        "        self.data_size = args.data_size\n",
        "\n",
        "    def extract(self):\n",
        "        cnn = models.vgg16(pretrained=True)\n",
        "        cnn.classifier = nn.Sequential(*[cnn.classifier[i] for i in range(5)])\n",
        "        cnn = cnn.to(device).eval()\n",
        "        generated_features = []\n",
        "        real_features = []\n",
        "        generated_img_paths = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            generated_data = ImageDataset(self.generated_dir, self.data_size, self.batch_size)\n",
        "            generated_loader = DataLoader(generated_data, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            for imgs, img_paths in tqdm(generated_loader, ncols=80):\n",
        "                target_features = cnn(imgs)\n",
        "\n",
        "                img_paths = list(img_paths)\n",
        "                generated_img_paths.extend(img_paths)\n",
        "\n",
        "                for target_feature in torch.chunk(target_features, target_features.size(0), dim=0):\n",
        "                    generated_features.append(target_feature)\n",
        "\n",
        "            real_data = ImageDataset(self.real_dir, self.data_size, self.batch_size)\n",
        "            real_loader = DataLoader(real_data, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            for imgs, _ in tqdm(real_loader, ncols=80):\n",
        "                target_features = cnn(imgs)\n",
        "\n",
        "                for target_feature in torch.chunk(target_features, target_features.size(0), dim=0):\n",
        "                    real_features.append(target_feature)\n",
        "\n",
        "        return generated_features, real_features, generated_img_paths\n",
        "\n",
        "    def show_image(self, img):\n",
        "        unloader = transforms.ToPILImage()\n",
        "        plt.ion()\n",
        "        plt.figure()\n",
        "        image = img.cpu().clone()\n",
        "        image = image.squeeze(0)\n",
        "        image = unloader(image)\n",
        "        plt.imshow(image)\n",
        "        plt.title(Image)\n",
        "        plt.pause(10)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dir_path, data_size=100, batch_size=64):\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "        data_size = data_size - data_size%batch_size\n",
        "\n",
        "        self.img_paths = []\n",
        "\n",
        "        for i, img_name in enumerate(os.listdir(dir_path)):\n",
        "            if i >= data_size:\n",
        "                break\n",
        "            img_path = os.path.join(dir_path, img_name)\n",
        "            self.img_paths.append(img_path)\n",
        "\n",
        "        self.imsize = 224\n",
        "\n",
        "        self.transformations = transforms.Compose([\n",
        "            transforms.Resize(self.imsize),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]),\n",
        "            transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961],\n",
        "            std=[1,1,1]),\n",
        "            transforms.Lambda(lambda x: x.mul_(255)),\n",
        "            ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        image = self.transformations(image)\n",
        "        return image.to(device, torch.float), img_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvcVE7kdFJH1"
      },
      "source": [
        "import os, torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import operator\n",
        "from tqdm import tqdm\n",
        "\n",
        "class precision_and_recall(object):\n",
        "    def __init__(self, args):\n",
        "        # parameters\n",
        "        self.args = args\n",
        "        self.result_dir = args.result_dir\n",
        "        self.batch_size = args.batch_size\n",
        "        self.cpu = args.cpu\n",
        "        self.data_size = args.data_size\n",
        "        self.k = 3\n",
        "\n",
        "    def run(self):\n",
        "        \n",
        "        # load data using vgg16\n",
        "        extractor = feature_extractor(self.args)\n",
        "        generated_features, real_features, _ = extractor.extract()\n",
        "\n",
        "        # equal number of samples\n",
        "        data_num = min(len(generated_features), len(real_features))\n",
        "        print(f'data num: {data_num}')\n",
        "\n",
        "        if data_num <= 0:\n",
        "            print(\"there is no data\")\n",
        "            return\n",
        "        generated_features = generated_features[:data_num]\n",
        "        real_features = real_features[:data_num]\n",
        "\n",
        "        # get precision and recall\n",
        "        precision = self.manifold_estimate(real_features, generated_features, self.k)\n",
        "        recall = self.manifold_estimate(generated_features, real_features, self.k)\n",
        " \n",
        "        print(\"Precision is: \" + str(precision))        \n",
        "        print(\"Recall is   : \" + str(recall))\n",
        "\n",
        "    def manifold_estimate(self, A_features, B_features, k):\n",
        "        \n",
        "        KNN_list_in_A = {}\n",
        "        for A in tqdm(A_features, ncols=80):\n",
        "            pairwise_distances = np.zeros(shape=(len(A_features)))\n",
        "\n",
        "            for i, A_prime in enumerate(A_features):\n",
        "                d = torch.norm((A-A_prime), 2)\n",
        "                pairwise_distances[i] = d\n",
        "\n",
        "            v = np.partition(pairwise_distances, k)[k]\n",
        "            KNN_list_in_A[A] = v\n",
        "\n",
        "        n = 0 \n",
        "\n",
        "        for B in tqdm(B_features, ncols=80):\n",
        "            for A_prime in A_features:\n",
        "                d = torch.norm((B-A_prime), 2)\n",
        "                if d <= KNN_list_in_A[A_prime]:\n",
        "                    n+=1\n",
        "                    break\n",
        "\n",
        "        return n/len(B_features)\n",
        "\n",
        "class realism(object):\n",
        "    def __init__(self, args):\n",
        "        # parameters\n",
        "        self.args = args\n",
        "        # self.data_dir = args.data_dir\n",
        "        self.result_dir = args.result_dir\n",
        "        self.batch_size = args.batch_size\n",
        "        self.cpu = args.cpu\n",
        "        self.k = 3  \n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        # load data using vgg16\n",
        "        extractor = feature_extractor(self.args)\n",
        "        generated_features, real_features, generated_img_paths = extractor.extract()\n",
        "\n",
        "        # equal number of samples\n",
        "        data_num = min(len(generated_features), len(real_features))\n",
        "        print(f'data num: {data_num}')\n",
        "\n",
        "        if data_num <= 0:\n",
        "            print(\"there is no data\")\n",
        "            return\n",
        "        generated_features = generated_features[:data_num]\n",
        "        real_features = real_features[:data_num]\n",
        "        generated_img_paths = generated_img_paths[:data_num]\n",
        "\n",
        "        KNN_list_in_real = self.calculate_real_NNK(real_features, self.k, data_num)\n",
        "\n",
        "        for i, generated_feature in enumerate(tqdm(generated_features, ncols=80)):\n",
        "\n",
        "            max_value = 0\n",
        "            for real_feature, KNN_radius in KNN_list_in_real:\n",
        "                d = torch.norm((real_feature-generated_feature), 2)\n",
        "                value = KNN_radius/d\n",
        "                if max_value < value:\n",
        "                    max_value = value\n",
        "\n",
        "            # print images with specific names\n",
        "            if 'high_realism' in generated_img_paths[i] or 'low_realism' in generated_img_paths[i]:\n",
        "                print(f'{generated_img_paths[i]} realism score: {max_value}')\n",
        "\n",
        "        return\n",
        "\n",
        "    def calculate_real_NNK(self, real_features, k, data_num):\n",
        "        KNN_list_in_real = {}\n",
        "        for real_feature in tqdm(real_features, ncols=80):\n",
        "            pairwise_distances = np.zeros(shape=(len(real_features)))\n",
        "\n",
        "            for i, real_prime in enumerate(real_features):\n",
        "                d = torch.norm((real_feature-real_prime), 2)\n",
        "                pairwise_distances[i] = d\n",
        "\n",
        "            v = np.partition(pairwise_distances, k)[k]\n",
        "            KNN_list_in_real[real_feature] = v\n",
        "\n",
        "        # remove half of larger values\n",
        "        KNN_list_in_real = sorted(KNN_list_in_real.items(), key=operator.itemgetter(1)) \n",
        "        KNN_list_in_real = KNN_list_in_real[:int(data_num/2)]\n",
        "\n",
        "\n",
        "        return KNN_list_in_real"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY5MKsL6Ic9p",
        "outputId": "66074a0e-deae-4e14-929f-ffeb44a4fa9f"
      },
      "source": [
        "# Sketch thing I have to do since I'm running in colab - Avi\n",
        "!rmdir /content/fake_data/.ipynb_checkpoints\n",
        "!rmdir /content/real_data/.ipynb_checkpoints"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rmdir: failed to remove '/content/fake_data/.ipynb_checkpoints': No such file or directory\n",
            "rmdir: failed to remove '/content/real_data/.ipynb_checkpoints': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6-hrHA33X0r",
        "outputId": "e08cd86f-539e-4a10-a2dd-af4e86359ada"
      },
      "source": [
        "import argparse, os, torch\n",
        "\n",
        "G_DIRECTORY = '/content/fake_data'\n",
        "R_DIRECTORY = '/content/real_data'\n",
        "\n",
        "def parse_args():\n",
        "    desc = \"calcualte precision and recall OR realism\"\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('--cal_type', type=str, default='precision_and_recall', choices=['precision_and_recall', 'realism'], help='The type of calcualtion')\n",
        "    \n",
        "    parser.add_argument('--result_dir', type=str, default='results', help='Directory name to save the model')\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='The size of batch')\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "    parser.add_argument('--seed', type=int, default=0)\n",
        "    parser.add_argument('--data_size', type=int, default=4)\n",
        "\n",
        "    parser.add_argument('--generated_dir', default=G_DIRECTORY)\n",
        "    parser.add_argument('--real_dir', default=R_DIRECTORY)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    # print(args)\n",
        "    return check_args(args)\n",
        "\n",
        "\n",
        "def check_args(args):\n",
        "    # --result_dir\n",
        "    if not os.path.exists(args.result_dir):\n",
        "        os.makedirs(args.result_dir)    \n",
        "    # --batch_size\n",
        "    try:\n",
        "        assert args.batch_size >= 1\n",
        "    except:\n",
        "        print('batch size must be larger than or equal to one')\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    # parse arguments\n",
        "    args = parse_args()\n",
        "\n",
        "    if args.cal_type == 'precision_and_recall':\n",
        "        task = precision_and_recall(args)\n",
        "    else:\n",
        "        task = realism(args)\n",
        "\n",
        "    task.run()\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()   "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 32.15it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 26.72it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 199.62it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 763.26it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 624.87it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 837.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data num: 4\n",
            "Precision is: 1.0\n",
            "Recall is   : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}